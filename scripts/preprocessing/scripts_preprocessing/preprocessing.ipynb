{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rxnmapper\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from rdkit import RDLogger\n",
    "from rxnmapper import RXNMapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    pattern = \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def canno(smi):\n",
    "    return Chem.MolToSmiles(Chem.MolFromSmiles(smi))\n",
    "\n",
    "def rxnF2onmt(reactionF, outpath, ttv=\"train\",canno=False):\n",
    "    assert ttv in [\"train\", \"test\", \"val\"]\n",
    "\n",
    "    with open(reactionF,\"r\") as f:\n",
    "        reactions = f.readlines()\n",
    "        reactions = [i.rstrip() for i in reactions]\n",
    "    \n",
    "    srcF = open(f\"{outpath}/src-{ttv}.txt\",\"w\")\n",
    "    tgtF = open(f\"{outpath}/tgt-{ttv}.txt\",\"w\")\n",
    "    \n",
    "    reactants = []  \n",
    "    products = []\n",
    "\n",
    "    for reaction in tqdm(reactions):\n",
    "\n",
    "        try:\n",
    "            reactant, product = reaction.split('>>')\n",
    "            if canno:\n",
    "                reactant = Chem.MolToSmiles(Chem.MolFromSmarts(reactant))\n",
    "                product = Chem.MolToSmiles(Chem.MolFromSmarts(product))\n",
    "\n",
    "            reactant = smi_tokenizer(reactant)\n",
    "            reactants.append(reactant)\n",
    "            product = smi_tokenizer(product)\n",
    "            products.append(product)\n",
    "        except:\n",
    "            # print('wrong:', reaction)\n",
    "            continue\n",
    "\n",
    "    for i, v in enumerate(reactants):\n",
    "        if products[i] != '' and reactants[i] != '':\n",
    "            srcF.write(products[i] + '\\n')\n",
    "            tgtF.write(reactants[i] + '\\n')\n",
    "\n",
    "    srcF.close(), tgtF.close()\n",
    "\n",
    "def rxnF2rtfm(file_path:str,output_path:str,ttv=\"train\"):\n",
    "    rxn_mapper = RXNMapper()\n",
    "    assert ttv in [\"train\", \"test\", \"val\"]\n",
    "\n",
    "    with open(file_path,\"r\") as f:\n",
    "        dl = f.readlines()\n",
    "        dl = [i.rstrip() for i in dl]\n",
    "    targets = []\n",
    "    results = [] \n",
    "    error = []\n",
    "    for i, row in enumerate(tqdm(dl)):\n",
    "        targets.append(row)\n",
    "        try:\n",
    "            results += rxn_mapper.get_attention_guided_atom_maps([row])\n",
    "            if i % 10000 == 0:\n",
    "                results_df = pd.DataFrame({'mapped_rxn': results})\n",
    "                print()\n",
    "                results_df['mapped_rxn'].to_csv(f\"{output_path}/raw_{ttv}.csv\")\n",
    "        except:\n",
    "            results += [{'mapped_rxn': f'{row}','confidence': 0}]\n",
    "            error.append((i,row))\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    newdf = pd.DataFrame({\"id\":[i for i in range(len(results_df))],\"class\":[\"UNK\" for i in range(len(results_df))],\"reactants>reagents>production\": results_df['mapped_rxn'],\"origin_SMILES\": targets})\n",
    "    am_newdf = newdf[newdf['origin_SMILES']!= newdf['reactants>reagents>production']]\n",
    "    am_newdf.to_csv(f\"{output_path}/raw_{ttv}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the lines from the original dataset\n",
    "%cd example\n",
    "with open('origin_dataset.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Shuffle the lines for randomness\n",
    "import random\n",
    "random.shuffle(lines)\n",
    "\n",
    "# Calculate the number of lines for each set\n",
    "total_lines = len(lines)\n",
    "train_size = int(0.9 * total_lines)\n",
    "test_size = int(0.05 * total_lines)\n",
    "\n",
    "# Split the dataset\n",
    "train_set, test_valid_set = train_test_split(lines, train_size=train_size, shuffle=False)\n",
    "test_set, valid_set = train_test_split(test_valid_set, train_size=test_size, shuffle=False)\n",
    "\n",
    "# Write the split datasets to separate files\n",
    "with open('train.txt', 'w') as file:\n",
    "    file.writelines(train_set)\n",
    "\n",
    "with open('test.txt', 'w') as file:\n",
    "    file.writelines(test_set)\n",
    "\n",
    "with open('valid.txt', 'w') as file:\n",
    "    file.writelines(valid_set)\n",
    "\n",
    "! wc -l *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84557 4697 4699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84557/84557 [00:36<00:00, 2319.94it/s]\n",
      "100%|██████████| 4697/4697 [00:02<00:00, 2281.41it/s]\n",
      "100%|██████████| 4699/4699 [00:02<00:00, 2323.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79885 4442 4458\n",
      "79852 4442 4458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79852/79852 [00:00<00:00, 2433506.96it/s]\n",
      "100%|██████████| 4442/4442 [00:00<00:00, 1965098.45it/s]\n",
      "100%|██████████| 4458/4458 [00:00<00:00, 1724926.87it/s]\n"
     ]
    }
   ],
   "source": [
    "target_path = '/path/to/READRetro/scripts/preprocessing/example'\n",
    "\n",
    "data_train = f\"{target_path}/train.txt\"\n",
    "with open(data_train) as f:\n",
    "    ds_train = f.readlines()\n",
    "    ds_train = [l.rstrip() for l in ds_train]\n",
    "\n",
    "data_test = f\"{target_path}/test.txt\"\n",
    "with open(data_test) as f:\n",
    "    ds_test = f.readlines()\n",
    "    ds_test = [l.rstrip() for l in ds_test]\n",
    "\n",
    "data_valid = f\"{target_path}/valid.txt\"\n",
    "with open(data_valid) as f:\n",
    "    ds_valid = f.readlines()\n",
    "    ds_valid = [l.rstrip() for l in ds_valid]\n",
    "\n",
    "print(len(ds_train),len(ds_test),len(ds_valid))\n",
    "\n",
    "cano_ds_train, cano_ds_test, cano_ds_valid = [],[],[]\n",
    "\n",
    "for line in tqdm(ds_train):\n",
    "    r,p = line.split(\">>\")\n",
    "    try:\n",
    "        cr,cp = canno(r), canno(p)\n",
    "    except: continue\n",
    "    cano_ds_train.append(f\"{cr}>>{cp}\")\n",
    "    \n",
    "for line in tqdm(ds_test):\n",
    "    r,p = line.split(\">>\")\n",
    "    try:\n",
    "        cr,cp = canno(r), canno(p)\n",
    "    except: continue\n",
    "    cano_ds_test.append(f\"{cr}>>{cp}\")\n",
    "\n",
    "for line in tqdm(ds_valid):\n",
    "    r,p = line.split(\">>\")\n",
    "    try:\n",
    "        cr,cp = canno(r), canno(p)\n",
    "    except: continue\n",
    "    cano_ds_valid.append(f\"{cr}>>{cp}\")\n",
    "\n",
    "\n",
    "print(len(cano_ds_train),len(cano_ds_test),len(cano_ds_valid))\n",
    "\n",
    "drop_cano_ds_train = list(pd.DataFrame({\"t\":cano_ds_train}).drop_duplicates()['t'])\n",
    "drop_cano_ds_test = list(pd.DataFrame({\"t\":cano_ds_test}).drop_duplicates()['t'])\n",
    "drop_cano_ds_valid = list(pd.DataFrame({\"t\":cano_ds_valid}).drop_duplicates()['t'])\n",
    "print(len(drop_cano_ds_train),len(drop_cano_ds_test),len(drop_cano_ds_valid))\n",
    "\n",
    "with open(f\"{target_path}/new_train.txt\",\"w\") as nf:\n",
    "    for line in tqdm(drop_cano_ds_train):\n",
    "        nf.write(line+\"\\n\")\n",
    "\n",
    "with open(f\"{target_path}/new_test.txt\",\"w\") as nf:\n",
    "    for line in tqdm(drop_cano_ds_test):\n",
    "        nf.write(line+\"\\n\")\n",
    "\n",
    "with open(f\"{target_path}/new_valid.txt\",\"w\") as nf:\n",
    "    for line in tqdm(drop_cano_ds_valid):\n",
    "        nf.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79852/79852 [00:01<00:00, 47513.76it/s]\n",
      "100%|██████████| 4458/4458 [00:00<00:00, 52951.13it/s]\n",
      "100%|██████████| 4442/4442 [00:00<00:00, 52956.49it/s]\n"
     ]
    }
   ],
   "source": [
    "! mkdir $target_path/onmt\n",
    "rxnF2onmt(f\"{target_path}/new_train.txt\",f'{target_path}/onmt',ttv='train')\n",
    "rxnF2onmt(f\"{target_path}/new_valid.txt\",f'{target_path}/onmt',ttv='val')\n",
    "rxnF2onmt(f\"{target_path}/new_test.txt\",f'{target_path}/onmt',ttv='test')\n",
    "\n",
    "! cp -r $target_path/onmt $target_path/g2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/79852 [00:01<1:44:17, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 110/79852 [00:02<17:53, 74.30it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
      " 13%|█▎        | 10004/79852 [01:39<14:18, 81.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20000/79852 [03:19<09:59, 99.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 29999/79852 [05:00<07:51, 105.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 39998/79852 [06:41<07:15, 91.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 49993/79852 [08:22<04:55, 101.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 69990/79852 [11:42<01:40, 98.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79852/79852 [13:22<00:00, 99.50it/s] \n",
      "  0%|          | 9/4458 [00:00<00:54, 81.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 3279/4458 [00:32<00:11, 103.95it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 4458/4458 [00:43<00:00, 101.41it/s]\n",
      "  0%|          | 17/4442 [00:00<00:54, 81.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 195/4442 [00:01<00:39, 106.84it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 4442/4442 [00:43<00:00, 101.37it/s]\n"
     ]
    }
   ],
   "source": [
    "! mkdir $target_path/retroformer\n",
    "rxnF2rtfm(f\"{target_path}/new_train.txt\",f'{target_path}/retroformer',ttv=\"train\")\n",
    "rxnF2rtfm(f\"{target_path}/new_valid.txt\",f'{target_path}/retroformer',ttv=\"val\")\n",
    "rxnF2rtfm(f\"{target_path}/new_test.txt\",f'{target_path}/retroformer',ttv=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rxnmapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d8c61b43bf2b72edd1f19254b4790a7af534e3b33036b1dade4bfda4f0e17ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
